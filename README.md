<p align="center">

<img src="assets/Illustration.jpg" alt="MMJ-Bench"/>

</p>

# MMJ-Bench
MMJ-Bench is a comprehensive benchmark designed to systematically evaluate existing multi-modal jailbreak attacks and defenses in a unified manner. We present jailbreak attack and defense techniques evaluated in our paper in the following tables.


## Multi-model jailbreak attack
|                                                    **Method**                                                   |       **Source**      | **Key Properties**                                         | 
|:---------------------------------------------------------------------------------------------------------------:|:---------------------:|------------------------------------------------------------|
| FigStep | [FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts](https://arxiv.org/abs/2311.05608)  | Generation-based|
| MM-SafetyBench | [MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models](https://arxiv.org/abs/2311.17600) | Generation-based|
| VisualAdv | [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://ojs.aaai.org/index.php/AAAI/article/view/30150) | Optimization-based|
| ImgJP | [Jailbreaking Attack against Multimodal Large Language Model](https://arxiv.org/abs/2402.02309) | Optimization-based |
| AttackVLM | [On Evaluating Adversarial Robustness of Large Vision-Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a97b58c4f7551053b0512f92244b0810-Abstract-Conference.html) | Geneation-based
| Hades | [Images are achilles’ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models](https://arxiv.org/abs/2403.09792) | Generation-based|
|FC-Attack|[FC-Attack: Jailbreaking Large Vision-Language Models via Auto-Generated Flowcharts](https://arxiv.org/abs/2502.21059)|Generation-based|
| BEST-OF-N | [BEST-OF-N JAILBREAKING](https://arxiv.org/abs/2412.03556)|Generation-based|
| Arondight | [Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts](https://arxiv.org/abs/2407.15050)| Generation-based |
| ImgTrojan | [ImgTrojan: Jailbreaking Vision-Language Models with ONE Image](https://arxiv.org/abs/2403.02910) | Optimization-based |
| VisualGCG | [White-box Multimodal Jailbreaks Against Large Vision-Language Models](https://arxiv.org/abs/2405.17894) | Optimization-based |
| IDEATOR | [IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models Using Themselves](https://arxiv.org/abs/2411.00827) | Generation-based |
| Align is not Enough | [Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models](https://ieeexplore.ieee.org/abstract/document/10829683) | Optimization-based |
|JPPN| [Utilizing Jailbreak Probability to Attack and Safeguard Multimodal LLMs](https://arxiv.org/abs/2503.06989)|Optimization-based |
|SI-Attack| [Jailbreaking Multimodal Large Language Models via Shuffle Inconsistency](https://arxiv.org/abs/2501.04931)| Generation-based |
|MIRAGE| [MIRAGE: Multimodal Immersive Reasoning and Guided Exploration for Red-Team Jailbreak Attacks](https://arxiv.org/abs/2503.19134)| Generation-based |
|CS-DJ| [Distraction is All You Need for Multimodal Large Language Model Jailbreaking](https://arxiv.org/abs/2502.10794)| Optimization-based |

Our evaluation results of MLLMs jailbreak attacks across six models are as follows:

<p align="center">

<img src="assets/ASR_eval_comparison_models.jpg" alt="ASR"/>

</p>

## Multi-modal jailbreak defense
|                                                    **Method**                                                   |       **Source**      | **Key Properties**                                         | 
|:---------------------------------------------------------------------------------------------------------------:|:---------------------:|------------------------------------------------------------|
|VLGuard| [Safety fine-tuning at (almost) no cost: A baseline for vision large language models](https://www.research.ed.ac.uk/en/publications/safety-fine-tuning-at-almost-no-cost-a-baseline-for-vision-large-) | Proactive defense|
| Adashield | [Adashield: Safeguarding multimodal large language models from structure-based attack via adaptive shield prompting](https://arxiv.org/abs/2403.09513) | Reactice defense |
|JailGuard| [JailGuard: A Universal Detection Framework for LLM Prompt-based Attacks](https://arxiv.org/abs/2312.10766) | Reactive defense |
|CIDER| [Cross-modality Information Check for Detecting Jailbreaking in Multimodal Large Language Models](https://arxiv.org/abs/2407.21659) | Reactive defense |
| BlueSuffix | [BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks](https://arxiv.org/abs/2410.20971) | Reactive defense |
| ProEAT | [Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks](https://arxiv.org/abs/2503.04833) | Proactive defense |
| ShieldLearner | [ShieldLearner: A New Paradigm for Jailbreak Attack Defense in LLMs](https://arxiv.org/abs/2502.13162) | Reactive defense |
| HiddenDetect | [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744) | Reactive defense |
| Tit-for-Tat | [Tit-for-Tat: Safeguarding Large Vision-Language Models Against Jailbreak Attacks via Adversarial Defense](https://arxiv.org/abs/2503.11619) | Reactive defense |
| TAIJI | [TAIJI: Textual Anchoring for Immunizing Jailbreak Images in Vision Language Models](https://arxiv.org/abs/2503.10872) | Reactive defense |
## Usage
### Installation
```
conda create -n MMJ-Bench python=3.10
conda create MMJ-Bench
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```


### Step 1 - Generate Test Cases
In the first step, jailbreak attack techniques are used to generate test cases with `generate_test_cases.py`.
```
./scripts/generate_test_cases.sh $method_name $behaviors_path $save_dir
```


### Step 2 - Generate Completions
After generating test cases ， we can generate completions for a target model with or without defense techniques.

Without defense methods: 
```
./scripts/generate_completions.sh $model_name $behaviors_path $test_cases_path $save_path $max_new_tokens $incremental_update
```
With defense methods.
```
./scripts/generate_completions_defense.sh $attack_type $target_model $defense_type
```

### Step 3 - Evaluate Completions
After generate completions from a `target_model` from Step 2, We will utilize the classifier provided by HarmBench to label whether each completion is an example of its corresponding behavior.
```
./scripts/evaluate_completions.sh $cls_path $behaviors_path $completions_path $save_path
```

## Acknowledgement and citation
We thank the following open-source reposities.

    [1]  https://github.com/centerforaisafety/HarmBench
    [2]  https://github.com/thuccslab/figstep
    [3]  https://github.com/isXinLiu/MM-SafetyBench
    [4]  https://github.com/Unispac/Visual-Adversarial-Examples-Jailbreak-Large-Language-Models
    [5]  https://github.com/abc03570128/Jailbreaking-Attack-against-Multimodal-Large-Language-Model
    [6]  https://github.com/yunqing-me/AttackVLM
    [7]  https://github.com/AoiDragon/HADES
    [8]  https://github.com/shiningrain/JailGuard
    [9]  https://github.com/SaFoLab-WISC/AdaShield

